# This is the ConfigMap for Prometheus. Within it, you'll find `prometheus.yml`, which will be bound to the Prometheus pod. 
# Before starting Prometheus, we need to configure the endpoints from which we want to scrape data. Additionally,
# there is an `alerting-rules.yml` file responsible for alerting using Alertmanager. For better readability and understanding, I created a ConfigMap for the Prometheus pod.
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 30s
      evaluation_interval: 15s
    alerting:
      alertmanagers:
        - static_configs:
            - targets: ['alertmanager.monitoring.svc.cluster.local:9093']
    rule_files:
      - /etc/prometheus/alerting-rules.yaml
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
      - job_name: kube-state-metrics
        honor_timestamps: true
        scrape_interval: 1m
        scrape_timeout: 1m
        metrics_path: /metrics
        scheme: http
        static_configs:
        - targets:
          - kube-state-metrics.monitoring.svc.cluster.local:8080
        relabel_configs:
        - source_labels: [__address__]
          target_label: cluster
          replacement: kubernetes
      - job_name: 'kubelet'
        scheme: https
        metrics_path: /metrics/cadvisor
        tls_config:
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

  alerting-rules.yaml: |
    groups:
    - name: Infrastuctutre alerts
      rules:
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready", status="false"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
          description: "Node {{ $labels.node }} has been in a NotReady state for more than 5 minutes."
      - alert: LowNodeMemory
        expr: kube_node_status_allocatable{resource="memory"} < 1 * 1024 * 1024 * 1024  # less than 1 GB
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} has low allocatable memory"
          description: "Node {{ $labels.node }} has less than 1GB of allocatable memory."

